{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k_9PbX8xQHRR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PqL2OCT-Qc5Z"
   },
   "outputs": [],
   "source": [
    "def make_df(path_to_file, inp_size, out_size,num_data=500, examples=2):\n",
    "  #inp_size, out_size, examples = 15, 15, 4      # Modify\n",
    "  cols = (inp_size**2 + out_size**2) * examples\n",
    "  df = pd.DataFrame(columns=(list(range(cols))+['operation','output']))\n",
    "  op_list = [f'Dilation SE{i}' for i in range(1,9)] + [f'Erosion SE{i}' for i in range(1,9)]\n",
    "  #path_to_file = \"/content/IPARC_ChallengeV2/Dataset/CatA_Simple\"   # Modify\n",
    "\n",
    "  for id in range(num_data):\n",
    "    with open(f'{path_to_file}/Task{id:03}.json') as f:\n",
    "      data = json.load(f)\n",
    "    with open(f'{path_to_file}/Task{id:03}_soln.txt') as file:\n",
    "      lines = [line.rstrip() for line in file]\n",
    "    d = []\n",
    "    for i in range(len(data)):\n",
    "      d += [element for innerList in data[i]['input'] for element in innerList] + [element for innerList in data[i]['output'] for element in innerList]\n",
    "    for op in op_list:\n",
    "      df.loc[len(df)] = (d + [op, op in lines])\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'object' has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mmake_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Dataset/Sequence_4/CatA_Simple\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m, in \u001b[0;36mmake_df\u001b[0;34m(path_to_file, inp_size, out_size, num_data, examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_df\u001b[39m(path_to_file, inp_size, out_size,num_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;66;03m#inp_size, out_size, examples = 15, 15, 4      # Modify\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   cols \u001b[38;5;241m=\u001b[39m (inp_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m out_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m examples\n\u001b[0;32m----> 4\u001b[0m   df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   op_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDilation SE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m9\u001b[39m)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErosion SE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m9\u001b[39m)]\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;66;03m#path_to_file = \"/content/IPARC_ChallengeV2/Dataset/CatA_Simple\"   # Modify\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/frame.py:411\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    407\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    408\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns), dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 411\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43minit_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/internals/construction.py:242\u001b[0m, in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m             nan_dtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m--> 242\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_arraylike_from_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mloc[missing] \u001b[38;5;241m=\u001b[39m [val] \u001b[38;5;241m*\u001b[39m missing\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/dtypes/cast.py:1229\u001b[0m, in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[0;34m(value, length, dtype)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, (np\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mtype\u001b[39m(np\u001b[38;5;241m.\u001b[39mdtype))):\n\u001b[0;32m-> 1229\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;129;01mand\u001b[39;00m is_integer_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m isna(value):\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;66;03m# coerce if we have nan for an integer dtype\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'object' has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "df1 = make_df('../Dataset/Sequence_4/CatA_Simple', 15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_X_y(df):\n",
    "    \n",
    "    y = df.pop('output')\n",
    "    X = df\n",
    "    X = pd.get_dummies(df, columns = ['operation'])\n",
    "    X.columns = X.columns.astype(str)\n",
    "    return X, y\n",
    "\n",
    "def training(X, y, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    print('Logistic Regression')\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'train accuracy = {clf.score(X_train, y_train)}, test accuracy = {clf.score(X_test, y_test)} \\n')\n",
    "    \n",
    "    print('Decision Tree')\n",
    "    dt = DecisionTreeClassifier(random_state=0)\n",
    "    dt.fit(X_train, y_train)\n",
    "    print(f'train accuracy = {dt.score(X_train, y_train)}, test accuracy = {dt.score(X_test, y_test)} \\n')\n",
    "\n",
    "    print('Support Vector Classifier')\n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(X_train, y_train)\n",
    "    print(f'train accuracy = {svc.score(X_train, y_train)}, test accuracy = {svc.score(X_test, y_test)} \\n')\n",
    "\n",
    "    print('Random Forest')\n",
    "    rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(f'train accuracy = {rf.score(X_train, y_train)}, test accuracy = {rf.score(X_test, y_test)} \\n')\n",
    "\n",
    "    print('Gradient Boosting')\n",
    "    gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "    gbc.fit(X_train, y_train)\n",
    "    print(f'train accuracy = {gbc.score(X_train, y_train)}, test accuracy = {gbc.score(X_test, y_test)} \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEJAS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.76875, test accuracy = 0.76875 \n",
      "\n",
      "Decision Tree\n",
      "train accuracy = 1.0, test accuracy = 0.653125 \n",
      "\n",
      "Support Vector Classifier\n",
      "train accuracy = 0.76796875, test accuracy = 0.771875 \n",
      "\n",
      "Random Forest\n",
      "train accuracy = 0.76796875, test accuracy = 0.771875 \n",
      "\n",
      "Gradient Boosting\n",
      "train accuracy = 0.76875, test accuracy = 0.76875 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = make_X_y(df1)\n",
    "training(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
